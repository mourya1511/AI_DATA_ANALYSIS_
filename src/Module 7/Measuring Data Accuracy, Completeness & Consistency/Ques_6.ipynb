{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture to Monitor Data Quality Over Time\n",
    "\n",
    "**Description**: Design a monitoring system in Python that checks and logs data quality metrics (accuracy, completeness) for a dataset over time.\n",
    "\n",
    "**Steps to follow:**\n",
    "1. Implement a Scheduled Script:\n",
    "    - Use schedule library to periodically run a script.\n",
    "2. Script to Calculate Metrics:\n",
    "    - For simplicity, use a function calculate_quality_metrics() that calculates and logs metrics such as missing rate or mismatch rate.\n",
    "3. Store Logs:\n",
    "    - Use Python's logging library to save these metrics over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Starting Data Quality Monitor... Press Ctrl+C to stop.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 124\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     schedule\u001b[38;5;241m.\u001b[39mrun_pending()\n\u001b[0;32m--> 124\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import schedule\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# ----------------------------\n",
    "# Setup Logging (buffered I/O)\n",
    "# ----------------------------\n",
    "logging.basicConfig(\n",
    "    filename='quality_metrics.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Improved Quality Metrics Function\n",
    "# ----------------------------\n",
    "def calculate_quality_metrics(df: pd.DataFrame, expected_schema: dict = None) -> dict:\n",
    "    metrics = {}\n",
    "\n",
    "    # Check if the expected columns exist\n",
    "    if expected_schema:\n",
    "        missing_columns = set(expected_schema.keys()) - set(df.columns)\n",
    "        if missing_columns:\n",
    "            logging.warning(f\"Missing expected columns: {missing_columns}\")\n",
    "            metrics[\"missing_columns\"] = list(missing_columns)\n",
    "\n",
    "    # 1. Column-wise missing value rate\n",
    "    try:\n",
    "        missing_rate = df.isnull().mean().to_dict()\n",
    "        metrics[\"missing_rate\"] = missing_rate\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calculating missing rate: {e}\")\n",
    "    \n",
    "    # 2. Row-wise missing value fraction\n",
    "    try:\n",
    "        metrics[\"row_missing_fraction\"] = (df.isnull().any(axis=1).sum() / len(df)) if len(df) > 0 else 0\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calculating row missing fraction: {e}\")\n",
    "\n",
    "    # 3. Schema mismatches (type checks)\n",
    "    if expected_schema:\n",
    "        mismatches = {}\n",
    "        for col, dtype in expected_schema.items():\n",
    "            if col not in df.columns:\n",
    "                mismatches[col] = \"missing column\"\n",
    "            elif not np.issubdtype(df[col].dtype, np.dtype(dtype)):\n",
    "                mismatches[col] = f\"type mismatch (expected {dtype}, found {df[col].dtype})\"\n",
    "        metrics[\"schema_mismatches\"] = mismatches\n",
    "    else:\n",
    "        metrics[\"schema_mismatches\"] = {}\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# ----------------------------\n",
    "# Load or Simulate Dataset\n",
    "# ----------------------------\n",
    "def load_data():\n",
    "    data = load_diabetes()\n",
    "    df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "    # Simulate missing values and anomaly\n",
    "    df.loc[0:5, 'age'] = None\n",
    "    df.loc[10, 'bmi'] = None\n",
    "    df['fake_column'] = 'oops'  # unexpected column\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# Monitoring Job\n",
    "# ----------------------------\n",
    "def job():\n",
    "    logging.info(\"Running data quality monitoring job.\")\n",
    "\n",
    "    try:\n",
    "        # Load data snapshot\n",
    "        df = load_data()\n",
    "\n",
    "        # Replace infinite values with NaN\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "        # Drop columns with all missing\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "        # Define expected schema\n",
    "        expected_schema = {\n",
    "            'age': 'float64',\n",
    "            'bmi': 'float64',\n",
    "            'bp': 'float64',\n",
    "            's1': 'float64'\n",
    "        }\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = calculate_quality_metrics(df, expected_schema)\n",
    "\n",
    "        # Log each metric (only log if needed)\n",
    "        if metrics[\"missing_rate\"]:\n",
    "            logging.info(\"Missing Rate per column: %s\", metrics[\"missing_rate\"])\n",
    "        if metrics[\"row_missing_fraction\"] > 0:\n",
    "            logging.info(\"Row Missing Fraction: %.4f\", metrics[\"row_missing_fraction\"])\n",
    "        if metrics[\"schema_mismatches\"]:\n",
    "            logging.info(\"Schema Mismatches: %s\", metrics[\"schema_mismatches\"])\n",
    "\n",
    "        logging.info(\"Job completed.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Monitoring job failed: {e}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Schedule the Job\n",
    "# ----------------------------\n",
    "schedule.every(10).seconds.do(job)  # Run every 10 seconds\n",
    "\n",
    "# ----------------------------\n",
    "# Run Loop\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ“Š Starting Data Quality Monitor... Press Ctrl+C to stop.\")\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
