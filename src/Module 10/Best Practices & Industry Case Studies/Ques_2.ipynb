{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing ML Model Monitoring Pipelines\n",
    "\n",
    "### Model Performance Drift:\n",
    "**Description**: Setup a monitoring pipeline to track key performance metrics (e.g., accuracy, precision) of an ML model over time using a monitoring tool or dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F./home/vscode/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      ".\n",
      "======================================================================\n",
      "FAIL: test_anomaly_detection (__main__.TestMonitoringFunctions)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_75325/1438541973.py\", line 122, in test_anomaly_detection\n",
      "    self.assertGreaterEqual(len(anomalies), 2)\n",
      "AssertionError: 1 not greater than or equal to 2\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.008s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Monitoring Pipeline ===\n",
      "[Performance] Accuracy: 0.50, Precision: 0.50\n",
      "[Drift] Feature 'f0' has drifted (p=0.0000)\n",
      "[Drift] Feature 'f1' has drifted (p=0.0000)\n",
      "[Drift] Feature 'f2' has drifted (p=0.0000)\n",
      "[Drift] Feature 'f3' has drifted (p=0.0053)\n",
      "[Drift] Feature 'f4' has drifted (p=0.0000)\n",
      "[Anomaly Detection] 1 anomalies detected\n",
      "\n",
      "=== Running Unit Tests ===\n",
      "[Anomaly Detection] 1 anomalies detected\n",
      "[Drift] Feature 'a' has drifted (p=0.0000)\n",
      "[Performance] Accuracy: 0.33, Precision: 0.00\n",
      "[ALERT] Drift in accuracy: baseline=0.90, current=0.33\n",
      "[ALERT] Drift in precision: baseline=0.80, current=0.00\n"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.datasets import make_classification\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import zscore\n",
    "import unittest\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Generate Base & Live Data\n",
    "# -------------------------------\n",
    "\n",
    "def generate_dataset(seed=42, drift=False, anomaly=False):\n",
    "    np.random.seed(seed)\n",
    "    X, y = make_classification(n_samples=1000, n_features=5, random_state=seed)\n",
    "\n",
    "    if drift:\n",
    "        X[:, 0] = X[:, 0] + np.random.normal(2, 0.5, size=X.shape[0])  # Introduce drift in feature 0\n",
    "\n",
    "    if anomaly:\n",
    "        y[:20] = 1 - y[:20]  # Flip predictions to simulate bad model outputs\n",
    "\n",
    "    return pd.DataFrame(X, columns=[f'f{i}' for i in range(X.shape[1])]), pd.Series(y, name='target')\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Train a Baseline Model\n",
    "# -------------------------------\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Monitoring Functions\n",
    "# -------------------------------\n",
    "\n",
    "# Performance Drift: Accuracy & Precision monitoring\n",
    "def monitor_model_performance(y_true, y_pred, baseline_metrics):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    drift_detected = False\n",
    "\n",
    "    print(f\"[Performance] Accuracy: {acc:.2f}, Precision: {prec:.2f}\")\n",
    "    for metric, current in zip(['accuracy', 'precision'], [acc, prec]):\n",
    "        baseline = baseline_metrics[metric]\n",
    "        if abs(current - baseline) > 0.1:  # threshold for drift\n",
    "            print(f\"[ALERT] Drift in {metric}: baseline={baseline:.2f}, current={current:.2f}\")\n",
    "            drift_detected = True\n",
    "    return drift_detected\n",
    "\n",
    "# Feature Distribution Drift (KS test)\n",
    "def monitor_feature_drift(X_train, X_live, threshold=0.05):\n",
    "    drift_report = {}\n",
    "    for col in X_train.columns:\n",
    "        stat, p_value = ks_2samp(X_train[col], X_live[col])\n",
    "        drift_report[col] = (p_value < threshold)\n",
    "        if p_value < threshold:\n",
    "            print(f\"[Drift] Feature '{col}' has drifted (p={p_value:.4f})\")\n",
    "    return drift_report\n",
    "\n",
    "# Anomaly Detection (Z-score of predicted probabilities)\n",
    "def detect_prediction_anomalies(pred_probs, z_thresh=2.5):\n",
    "    z_scores = np.abs(zscore(pred_probs))\n",
    "    anomalies = np.where(z_scores > z_thresh)[0]\n",
    "    print(f\"[Anomaly Detection] {len(anomalies)} anomalies detected\")\n",
    "    return anomalies.tolist()\n",
    "\n",
    "# -------------------------------\n",
    "# Run Simulation\n",
    "# -------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Training phase\n",
    "    X_train, y_train = generate_dataset()\n",
    "    X_val, y_val = generate_dataset(seed=100)  # Slightly different data for validation\n",
    "    model = train_model(X_train, y_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # Save baseline metrics\n",
    "    baseline_metrics = {\n",
    "        'accuracy': accuracy_score(y_val, y_val_pred),\n",
    "        'precision': precision_score(y_val, y_val_pred)\n",
    "    }\n",
    "\n",
    "    # Monitoring Phase - Live (with drift and anomalies)\n",
    "    X_live, y_live = generate_dataset(seed=200, drift=True, anomaly=True)\n",
    "    y_pred_live = model.predict(X_live)\n",
    "    pred_probs = model.predict_proba(X_live)[:, 1]\n",
    "\n",
    "    print(\"\\n=== Monitoring Pipeline ===\")\n",
    "    monitor_model_performance(y_live, y_pred_live, baseline_metrics)\n",
    "    monitor_feature_drift(X_train, X_live)\n",
    "    detect_prediction_anomalies(pred_probs)\n",
    "\n",
    "# -------------------------------\n",
    "# Unit Tests for Monitoring\n",
    "# -------------------------------\n",
    "\n",
    "class TestMonitoringFunctions(unittest.TestCase):\n",
    "\n",
    "    def test_performance_drift_detection(self):\n",
    "        base = {'accuracy': 0.9, 'precision': 0.8}\n",
    "        # Simulate drop in performance\n",
    "        result = monitor_model_performance([1, 0, 1], [0, 0, 0], base)\n",
    "        self.assertTrue(result)\n",
    "\n",
    "    def test_feature_drift(self):\n",
    "        X1 = pd.DataFrame(np.random.normal(0, 1, (100, 3)), columns=['a', 'b', 'c'])\n",
    "        X2 = X1.copy()\n",
    "        X2['a'] += 5  # Add drift\n",
    "        drift = monitor_feature_drift(X1, X2)\n",
    "        self.assertTrue(drift['a'])\n",
    "\n",
    "    def test_anomaly_detection(self):\n",
    "        probs = np.array([0.1] * 98 + [0.99, 0.001])  # Inject anomalies\n",
    "        anomalies = detect_prediction_anomalies(probs)\n",
    "        self.assertGreaterEqual(len(anomalies), 2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n=== Running Unit Tests ===\")\n",
    "    unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Distribution Drift:\n",
    "**Description**: Monitor the distribution of your input features in deployed models to detect any significant shifts from training data distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection in Predictions:\n",
    "**DEscription**: Implement an anomaly detection mechanism to flag unusual model\n",
    "predictions. Simulate anomalies by altering input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
