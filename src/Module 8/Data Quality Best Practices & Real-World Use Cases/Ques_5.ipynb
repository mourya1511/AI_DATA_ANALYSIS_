{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI/ML â€“ Improving Model Performance with Clean Data\n",
    "\n",
    "**Task 1**: Data Preprocessing for Models\n",
    "\n",
    "**Objective**: Enhance data quality for better AI/ML outcomes.\n",
    "\n",
    "**Steps**:\n",
    "1. Choose a dataset for training an AI/ML model.\n",
    "2. Identify common data issues like null values, redundant features, or noisydata.\n",
    "3. Apply preprocessing methods such as imputation, normalization, or feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, X_test):\n",
    "    try:\n",
    "        numeric_features = [\"Age\", \"Income\"]\n",
    "        categorical_features = [\"Gender\"]\n",
    "\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ])\n",
    "\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", numeric_transformer, numeric_features),\n",
    "                (\"cat\", categorical_transformer, categorical_features)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        preprocessor.fit(X_train)\n",
    "        X_train_processed = preprocessor.transform(X_train)\n",
    "        X_test_processed = preprocessor.transform(X_test)\n",
    "        return X_train_processed, X_test_processed\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing: {e}\")\n",
    "        raise\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    try:\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.007s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class TestDataQualityPipeline(unittest.TestCase):\n",
    "\n",
    "    def test_imputation(self):\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        data = pd.DataFrame({\"Age\": [25, np.nan, 30]})\n",
    "        imputer = SimpleImputer(strategy=\"mean\")\n",
    "        imputed = imputer.fit_transform(data)\n",
    "        self.assertAlmostEqual(imputed[1][0], 27.5)\n",
    "\n",
    "    def test_model_training(self):\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        X = np.array([[1, 2], [2, 3], [3, 4]])\n",
    "        y = np.array([0, 1, 0])\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X, y)\n",
    "        self.assertEqual(len(model.coef_[0]), 2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature matrix (numpy array):\n",
      "[[-1.7251639  -1.10448156  0.          1.          0.        ]\n",
      " [ 0.         -0.12272017  1.          0.          0.        ]\n",
      " [ 0.34503278  0.          1.          0.          0.        ]\n",
      " [ 1.38013112  1.8408026   0.          0.          1.        ]\n",
      " [ 0.         -0.61360087  0.          1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Simulate dataset with nulls, redundant features, and mixed data types\n",
    "data = {\n",
    "    \"Age\": [25, np.nan, 35, 40, np.nan],\n",
    "    \"Income\": [50000, 60000, np.nan, 80000, 55000],\n",
    "    \"Gender\": [\"Male\", \"Female\", \"Female\", None, \"Male\"],\n",
    "    \"Purchased\": [1, 0, 1, 0, 1],\n",
    "    \"RedundantFeature\": [1, 1, 1, 1, 1]  # constant feature, redundant\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Drop redundant feature\n",
    "df_clean = df.drop(columns=[\"RedundantFeature\"])\n",
    "\n",
    "# Step 2: Define preprocessing for numeric and categorical features\n",
    "numeric_features = [\"Age\", \"Income\"]\n",
    "categorical_features = [\"Gender\"]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),       # fill nulls with mean\n",
    "    (\"scaler\", StandardScaler())                        # normalize numeric data\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # fill nulls with mode\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))     # one-hot encode\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply preprocessing\n",
    "X = df_clean.drop(columns=[\"Purchased\"])\n",
    "y = df_clean[\"Purchased\"]\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(\"Processed feature matrix (numpy array):\")\n",
    "print(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model performance WITHOUT preprocessing ===\n",
      "Accuracy: 0.000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       2.0\n",
      "           1       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       3.0\n",
      "   macro avg       0.00      0.00      0.00       3.0\n",
      "weighted avg       0.00      0.00      0.00       3.0\n",
      "\n",
      "\n",
      "=== Model performance WITH preprocessing ===\n",
      "Accuracy: 0.000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       2.0\n",
      "           1       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       3.0\n",
      "   macro avg       0.00      0.00      0.00       3.0\n",
      "weighted avg       0.00      0.00      0.00       3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Simulate dataset with nulls, redundant features, and mixed data types\n",
    "data = {\n",
    "    \"Age\": [25, np.nan, 35, 40, np.nan, 29, 33, 38, 45, 50],\n",
    "    \"Income\": [50000, 60000, np.nan, 80000, 55000, 52000, 48000, 75000, np.nan, 82000],\n",
    "    \"Gender\": [\"Male\", \"Female\", \"Female\", None, \"Male\", \"Female\", \"Male\", \"Female\", \"Male\", None],\n",
    "    \"Purchased\": [1, 0, 1, 0, 1, 0, 0, 1, 1, 0],\n",
    "    \"RedundantFeature\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # constant redundant feature\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(columns=[\"Purchased\"])\n",
    "y = df[\"Purchased\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# === Without Preprocessing ===\n",
    "# We drop the redundant feature but do not handle missing data or encoding\n",
    "X_train_no_prep = X_train.drop(columns=[\"RedundantFeature\"])\n",
    "X_test_no_prep = X_test.drop(columns=[\"RedundantFeature\"])\n",
    "\n",
    "# For model input without preprocessing, fill missing numerics with -1 and categorical with 'missing'\n",
    "X_train_no_prep = X_train_no_prep.copy()\n",
    "X_test_no_prep = X_test_no_prep.copy()\n",
    "\n",
    "X_train_no_prep[\"Age\"] = X_train_no_prep[\"Age\"].fillna(-1)\n",
    "X_test_no_prep[\"Age\"] = X_test_no_prep[\"Age\"].fillna(-1)\n",
    "\n",
    "X_train_no_prep[\"Income\"] = X_train_no_prep[\"Income\"].fillna(-1)\n",
    "X_test_no_prep[\"Income\"] = X_test_no_prep[\"Income\"].fillna(-1)\n",
    "\n",
    "X_train_no_prep[\"Gender\"] = X_train_no_prep[\"Gender\"].fillna(\"missing\")\n",
    "X_test_no_prep[\"Gender\"] = X_test_no_prep[\"Gender\"].fillna(\"missing\")\n",
    "\n",
    "# Convert categorical gender to numeric codes simply (without one-hot)\n",
    "X_train_no_prep[\"Gender\"] = X_train_no_prep[\"Gender\"].astype('category').cat.codes\n",
    "X_test_no_prep[\"Gender\"] = X_test_no_prep[\"Gender\"].astype('category').cat.codes\n",
    "\n",
    "model_no_prep = LogisticRegression()\n",
    "model_no_prep.fit(X_train_no_prep, y_train)\n",
    "y_pred_no_prep = model_no_prep.predict(X_test_no_prep)\n",
    "\n",
    "acc_no_prep = accuracy_score(y_test, y_pred_no_prep)\n",
    "print(\"=== Model performance WITHOUT preprocessing ===\")\n",
    "print(f\"Accuracy: {acc_no_prep:.3f}\")\n",
    "print(classification_report(y_test, y_pred_no_prep))\n",
    "\n",
    "\n",
    "# === With Preprocessing ===\n",
    "# Drop redundant feature\n",
    "X_train_prep = X_train.drop(columns=[\"RedundantFeature\"])\n",
    "X_test_prep = X_test.drop(columns=[\"RedundantFeature\"])\n",
    "\n",
    "numeric_features = [\"Age\", \"Income\"]\n",
    "categorical_features = [\"Gender\"]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a pipeline that applies preprocessing then LogisticRegression\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression())\n",
    "])\n",
    "\n",
    "model_pipeline.fit(X_train_prep, y_train)\n",
    "y_pred_prep = model_pipeline.predict(X_test_prep)\n",
    "\n",
    "acc_prep = accuracy_score(y_test, y_pred_prep)\n",
    "print(\"\\n=== Model performance WITH preprocessing ===\")\n",
    "print(f\"Accuracy: {acc_prep:.3f}\")\n",
    "print(classification_report(y_test, y_pred_prep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Evaluate Model Performance\n",
    "\n",
    "**Objective**: Assess the impact of data quality improvements on model performance.\n",
    "\n",
    "**Steps**:\n",
    "1. Train a simple ML model with and without preprocessing.\n",
    "2. Analyze and compare model performance metrics to evaluate the impact of data quality strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
