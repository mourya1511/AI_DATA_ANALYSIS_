{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI & Machine Learning for Data Quality\n",
    "**Description**: AI and machine learning can automate and enhance data quality checks by learning patterns and identifying anomalies more effectively than static rules.\n",
    "\n",
    "**Task 1**: Training a model to predict and flag unusual trend patterns in sales data that\n",
    "deviate from historical norms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Using clustering algorithms to detect duplicate records where entries are not\n",
    "exactly identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 06:37:01,807 - WARNING - Missing values imputed in 'value'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Scores: [ 0.25798967  0.3036381   0.31073271  0.30141526  0.258531   -0.07669632\n",
      "  0.05113088]\n",
      "Anomaly Predictions: [ 1  1  1  1  1 -1  1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-v] [-q] [--locals] [-f] [-c] [-b]\n",
      "                             [-k TESTNAMEPATTERNS]\n",
      "                             [tests ...]\n",
      "ipykernel_launcher.py: error: argument -f/--failfast: ignored explicit argument '/home/vscode/.local/share/jupyter/runtime/kernel-v331ad0828149bc81b57206b41355fca79a661f26a.json'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import logging\n",
    "import unittest\n",
    "\n",
    "# Configure logging for error tracking\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# Data preparation with error handling\n",
    "def prepare_data(df, column_name):\n",
    "    try:\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Input DataFrame is empty.\")\n",
    "        if column_name not in df.columns:\n",
    "            raise KeyError(f\"Column '{column_name}' not found in DataFrame.\")\n",
    "        if df[column_name].isnull().any():\n",
    "            df[column_name] = df[column_name].fillna(df[column_name].mean())\n",
    "            logging.warning(f\"Missing values imputed in '{column_name}'.\")\n",
    "        return df[column_name].values.reshape(-1, 1)\n",
    "    except (KeyError, TypeError, ValueError) as e:\n",
    "        logging.error(f\"prepare_data error: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Model training with error handling\n",
    "def train_anomaly_model(data, contamination=0.1):\n",
    "    try:\n",
    "        model = IsolationForest(contamination=contamination, random_state=42)\n",
    "        model.fit(data)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.error(f\"train_anomaly_model error: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Detect anomalies with error handling\n",
    "def detect_anomalies(model, data):\n",
    "    try:\n",
    "        scores = model.decision_function(data)\n",
    "        predictions = model.predict(data)\n",
    "        return scores, predictions\n",
    "    except Exception as e:\n",
    "        logging.error(f\"detect_anomalies error: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Complete anomaly detection pipeline\n",
    "def anomaly_detection(df, column_name):\n",
    "    try:\n",
    "        data = prepare_data(df, column_name)\n",
    "        model = train_anomaly_model(data)\n",
    "        scores, predictions = detect_anomalies(model, data)\n",
    "        return scores, predictions\n",
    "    except Exception as e:\n",
    "        logging.error(f\"anomaly_detection error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    # Example data with an outlier and missing value\n",
    "    data = {'value': [1, 2, 3, 4, 5, 1000, None]}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    scores, preds = anomaly_detection(df, 'value')\n",
    "\n",
    "    if scores is not None and preds is not None:\n",
    "        print(\"Anomaly Scores:\", scores)\n",
    "        print(\"Anomaly Predictions:\", preds)\n",
    "    else:\n",
    "        print(\"Anomaly detection failed. Check logs for details.\")\n",
    "\n",
    "\n",
    "# Unit tests\n",
    "class TestAnomalyDetection(unittest.TestCase):\n",
    "\n",
    "    def test_prepare_data_valid(self):\n",
    "        df = pd.DataFrame({'val': [1, 2, 3]})\n",
    "        result = prepare_data(df, 'val')\n",
    "        self.assertEqual(result.shape, (3, 1))\n",
    "\n",
    "    def test_prepare_data_missing_column(self):\n",
    "        df = pd.DataFrame({'a': [1, 2]})\n",
    "        with self.assertRaises(KeyError):\n",
    "            prepare_data(df, 'missing')\n",
    "\n",
    "    def test_prepare_data_empty(self):\n",
    "        df = pd.DataFrame()\n",
    "        with self.assertRaises(ValueError):\n",
    "            prepare_data(df, 'any')\n",
    "\n",
    "    def test_train_anomaly_model(self):\n",
    "        data = np.array([[1], [2], [3]])\n",
    "        model = train_anomaly_model(data)\n",
    "        self.assertIsNotNone(model)\n",
    "\n",
    "    def test_detect_anomalies(self):\n",
    "        data = np.array([[1], [2], [3]])\n",
    "        model = train_anomaly_model(data)\n",
    "        scores, preds = detect_anomalies(model, data)\n",
    "        self.assertEqual(len(scores), len(data))\n",
    "        self.assertEqual(len(preds), len(data))\n",
    "\n",
    "    def test_full_pipeline(self):\n",
    "        df = pd.DataFrame({'value': [1, 2, 3, 100]})\n",
    "        scores, preds = anomaly_detection(df, 'value')\n",
    "        self.assertIsNotNone(scores)\n",
    "        self.assertIsNotNone(preds)\n",
    "        self.assertEqual(len(scores), len(df))\n",
    "        self.assertEqual(len(preds), len(df))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 06:39:07,151 - ERROR - File not found: your_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: [Errno 2] No such file or directory: 'your_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Loads data from a CSV file.\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        return data\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found: {file_path}\")\n",
    "        raise e\n",
    "    except pd.errors.ParserError as e:\n",
    "        logging.error(f\"Parsing error while reading the file: {file_path}\")\n",
    "        raise e\n",
    "\n",
    "def validate_columns(data, required_columns):\n",
    "    \"\"\"Checks if required columns exist in the DataFrame.\"\"\"\n",
    "    missing_cols = [col for col in required_columns if col not in data.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "\n",
    "def handle_duplicates(data):\n",
    "    \"\"\"Removes duplicate rows from the DataFrame.\"\"\"\n",
    "    before = len(data)\n",
    "    data_deduped = data.drop_duplicates()\n",
    "    after = len(data_deduped)\n",
    "    logging.info(f\"Removed {before - after} duplicates.\")\n",
    "    return data_deduped\n",
    "\n",
    "def detect_anomalies(data, feature_cols):\n",
    "    \"\"\"Detects anomalies using Isolation Forest.\"\"\"\n",
    "    try:\n",
    "        clf = IsolationForest(contamination=0.05, random_state=42)\n",
    "        clf.fit(data[feature_cols])\n",
    "        data['anomaly'] = clf.predict(data[feature_cols])\n",
    "        # Convert -1 to True for anomalies\n",
    "        data['is_anomaly'] = data['anomaly'] == -1\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error during anomaly detection\")\n",
    "        raise e\n",
    "\n",
    "def train_classifier(data, feature_cols, target_col):\n",
    "    \"\"\"Trains a classifier to validate data labels.\"\"\"\n",
    "    try:\n",
    "        X = data[feature_cols]\n",
    "        y = data[target_col]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        # Example classifier\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        clf = RandomForestClassifier(random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        logging.info(f\"Classification report:\\n{report}\")\n",
    "        return clf\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error during classifier training\")\n",
    "        raise e\n",
    "\n",
    "def main(file_path):\n",
    "    \"\"\"Main pipeline for data quality assessment.\"\"\"\n",
    "    try:\n",
    "        data = load_data(file_path)\n",
    "        required_columns = ['feature1', 'feature2', 'label']\n",
    "        validate_columns(data, required_columns)\n",
    "        data = handle_duplicates(data)\n",
    "        data_with_anomalies = detect_anomalies(data, ['feature1', 'feature2'])\n",
    "        classifier = train_classifier(data_with_anomalies, ['feature1', 'feature2'], 'label')\n",
    "        print(\"Data quality assessment completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    file_path = 'your_data.csv'\n",
    "    main(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Potential duplicate groups detected:\n",
      "\n",
      "Cluster 0:\n",
      "    Name                Email    Phone\n",
      "John Doe     john@example.com 555-1234\n",
      " Jon Doe      jon@example.com 555-1234\n",
      "John Doe john.doe@example.com 555-1234\n",
      " J0hn Do     john@example.com 555-1234\n",
      "\n",
      "Cluster 1:\n",
      "       Name                  Email    Phone\n",
      " Jane Smith jane.smith@example.com 555-5678\n",
      "Janet Smith      janet@example.com 555-5678\n",
      " Jane Smyth     jane.s@example.com 555-5678\n",
      "\n",
      "Cluster 2:\n",
      "     Name                 Email    Phone\n",
      "Jake Long jake.long@example.com 555-8765\n",
      "Jack Long jack.long@example.com 555-8766\n",
      "Jake L0ng      jake@example.com 555-8765\n"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# 1. Sample dataset simulating slightly different duplicate records\n",
    "data = {\n",
    "    'Name': [\n",
    "        'John Doe', 'Jon Doe', 'Jane Smith', 'Janet Smith', 'Jake Long',\n",
    "        'Jack Long', 'John Doe', 'J0hn Do', 'Jane Smyth', 'Jake L0ng'\n",
    "    ],\n",
    "    'Email': [\n",
    "        'john@example.com', 'jon@example.com', 'jane.smith@example.com', 'janet@example.com',\n",
    "        'jake.long@example.com', 'jack.long@example.com', 'john.doe@example.com',\n",
    "        'john@example.com', 'jane.s@example.com', 'jake@example.com'\n",
    "    ],\n",
    "    'Phone': [\n",
    "        '555-1234', '555-1234', '555-5678', '555-5678', '555-8765',\n",
    "        '555-8766', '555-1234', '555-1234', '555-5678', '555-8765'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2. Convert categorical/text fields to numeric features\n",
    "# Vectorize names and emails with TF-IDF\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2,4))  # char ngrams for fuzzy similarity\n",
    "name_vec = vectorizer.fit_transform(df['Name'])\n",
    "email_vec = vectorizer.fit_transform(df['Email'])\n",
    "\n",
    "# 3. Encode phone numbers (simple numeric encoding after removing non-digits)\n",
    "df['Phone_Num'] = df['Phone'].str.replace(r'\\D', '', regex=True).astype(int).values.reshape(-1, 1)\n",
    "\n",
    "# 4. Combine all features into one feature matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "phone_scaled = StandardScaler().fit_transform(df[['Phone_Num']])\n",
    "phone_sparse = csr_matrix(phone_scaled)\n",
    "features = hstack([name_vec, email_vec, phone_sparse])\n",
    "\n",
    "# 5. Apply DBSCAN clustering to group similar records (eps controls similarity threshold)\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=1, metric='cosine')\n",
    "clusters = dbscan.fit_predict(features)\n",
    "\n",
    "df['Cluster'] = clusters\n",
    "\n",
    "# 6. Display clusters with more than one member (potential duplicates)\n",
    "duplicates = df.groupby('Cluster').filter(lambda x: len(x) > 1).sort_values('Cluster')\n",
    "\n",
    "print(\"üîç Potential duplicate groups detected:\")\n",
    "for cluster_id, group in duplicates.groupby('Cluster'):\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    print(group[['Name', 'Email', 'Phone']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Implementing classification models to validate data based on learned\n",
    "characteristics from labeled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         3\n",
      "   macro avg       1.00      1.00      1.00         3\n",
      "weighted avg       1.00      1.00      1.00         3\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1 0]\n",
      " [0 2]]\n",
      "\n",
      "New data validation predictions (0=Valid, 1=Invalid): [0 1]\n"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Mock labeled dataset (features + label)\n",
    "data = {\n",
    "    'feature1': [10, 15, 14, 10, 8, 50, 55, 52, 48, 60],\n",
    "    'feature2': [100, 110, 105, 95, 90, 300, 310, 305, 290, 320],\n",
    "    'feature3': [5, 7, 6, 5, 4, 20, 21, 19, 22, 25],\n",
    "    'label':    [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]  # 0=Valid, 1=Invalid\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2. Split dataset\n",
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Train classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 4. Evaluate model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# 5. Validate new data\n",
    "new_data = pd.DataFrame({\n",
    "    'feature1': [12, 53],\n",
    "    'feature2': [102, 295],\n",
    "    'feature3': [6, 18]\n",
    "})\n",
    "\n",
    "predictions = clf.predict(new_data)\n",
    "print(\"\\nNew data validation predictions (0=Valid, 1=Invalid):\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
